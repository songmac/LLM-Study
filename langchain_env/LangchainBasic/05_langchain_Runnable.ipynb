{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith API Key ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY ì •ë³´ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langsmithë¡œ í”„ë¡œì íŠ¸ ì¶”ì  ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "Runnable\n"
     ]
    }
   ],
   "source": [
    "from utils import logging\n",
    "\n",
    "logging.langsmith(\"Runnable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ìš©ì ì •ì˜ ì²´ì¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë¡œí† ì½œì´ë©°, ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ì¡´ ë°©ì‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='50ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 16, 'total_tokens': 19, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7ca5ab83-69ac-44c5-a1ac-0a4417208751-0', usage_metadata={'input_tokens': 16, 'output_tokens': 3, 'total_tokens': 19, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{num} ì˜ 10ë°°ëŠ”?\")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"num\": 5}) # ê¸°ë³¸\n",
    "chain.invoke(5) # 1ê°œì˜ ë³€ìˆ˜ë§Œ ìˆì„ ë•Œ ì´ë ‡ê²Œë„ ì‚¬ìš© ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable ê°ì²´ë¥¼ ì‚¬ìš©í•œ ë°©ì‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{num} ì˜ 10ë°°ëŠ”?\")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "runnable_chain = {\"num\": RunnablePassthrough()} | prompt | llm\n",
    "runnable_chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê¸°ì¡´ ë°©ì‹ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì…ë ¥ê°’ì„ ì „ë‹¬í•´ì•¼ í•˜ë©°, ë‹¨ìˆœí•˜ê²Œ íŠ¹ì • ë³€ìˆ˜ì— ê°’ì„ ë°”ë¡œ ì¹˜í™˜í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "Runnable ê°ì²´ë¥¼ ì´ìš©í•œ ë°©ì‹ì€ ì…ë ¥ê°’ì„ ê°€ê³µí•˜ê±°ë‚˜ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnablePassthrough ëŠ” ì…ë ¥ì„ ë³€ê²½í•˜ì§€ ì•Šê±°ë‚˜ ì¶”ê°€ í‚¤ë¥¼ ë”í•˜ì—¬ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- RunnablePassthrough() ê°€ ë‹¨ë…ìœ¼ë¡œ í˜¸ì¶œë˜ë©´, ë‹¨ìˆœíˆ ì…ë ¥ì„ ë°›ì•„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "- RunnablePassthrough.assign(...) ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œë˜ë©´, ì…ë ¥ì„ ë°›ì•„ assign í•¨ìˆ˜ì— ì „ë‹¬ëœ ì¶”ê°€ ì¸ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 10}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# runnable\n",
    "RunnablePassthrough().invoke({\"num\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='100ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 16, 'total_tokens': 19, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-650d0135-d18d-45a1-807e-043ec79d977c-0', usage_metadata={'input_tokens': 16, 'output_tokens': 3, 'total_tokens': 19, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_chain = {\"num\": RunnablePassthrough()} | prompt | llm\n",
    "runnable_chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough.assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 1, 'new_num': 3}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì…ë ¥ ê°’ìœ¼ë¡œ ë“¤ì–´ì˜¨ ê°’ì˜ key/value ìŒê³¼ ìƒˆë¡­ê²Œ í• ë‹¹ëœ key/value ìŒì„ í•©ì¹©ë‹ˆë‹¤.\n",
    "# ì…ë ¥ í‚¤ : num, í• ë‹¹(assign) í‚¤ : new_num\n",
    "(RunnablePassthrough.assign(new_num=lambda x: x[\"num\"] * 3)).invoke({\"num\": 1}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì¸ìŠ¤í„´ìŠ¤ëŠ” ì—¬ëŸ¬ Runnable ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Runnableparallel ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "runnable = RunnableParallel(\n",
    "    passed = RunnablePassthrough(),\n",
    "    extra = RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified = lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "# runnable ì¸ìŠ¤í„´ìŠ¤ì— {\"num\": 1} ë”•ì…”ë„ˆë¦¬ë¥¼ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ invoke ë©”ì†Œë“œë¥¼ í˜¸ì¶œ\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain ë„ RunnableParallel ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = (\n",
    "    {\"country\": RunnablePassthrough()}\n",
    "    | PromptTemplate.from_template(\"{country} ì˜ ìˆ˜ë„ëŠ”?\")\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "chain2 = (\n",
    "    {\"country\": RunnablePassthrough()}\n",
    "    | PromptTemplate.from_template(\"{country} ì˜ ë©´ì ì€?\")\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "combined_chain = RunnableParallel(capital=chain1, area=chain2)\n",
    "combined_chain.invoke(\"ëŒ€í•œë¯¼êµ­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableLambda ëŠ” ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ì‹¤í–‰ í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ í†µí•´ ê°œë°œìëŠ” ìì‹ ë§Œì˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³ , í•´ë‹¹ í•¨ìˆ˜ë¥¼ RunnableLambda ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„° ì „ì²˜ë¦¬, ê³„ì‚°, ë˜ëŠ” ì™¸ë¶€ APIì™€ì˜ ìƒí˜¸ ì‘ìš©ê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "#### ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ RunnableLambdaë¡œ ë˜í•‘í•˜ì—¬ í™œìš©í•  ìˆ˜ ìˆëŠ”ë°, ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ê°€ ë°›ì„ ìˆ˜ ìˆëŠ” ì¸ìëŠ” 1ê°œ ë¿ì´ë¼ëŠ” ì  ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë§Œì•½, ì—¬ëŸ¬ ì¸ìˆ˜ë¥¼ ë°›ëŠ” í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ê³  ì‹¶ë‹¤ë©´, ë‹¨ì¼ ì…ë ¥ì„ ë°›ì•„ë“¤ì´ê³  ì´ë¥¼ ì—¬ëŸ¬ ì¸ìˆ˜ë¡œ í’€ì–´ë‚´ëŠ” ë˜í¼ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "wrapperë€?\n",
    "- ë˜í¼(wrapper) í•¨ìˆ˜ë€, ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ê°ì‹¸ì„œ í˜¸ì¶œí•˜ê±°ë‚˜ ê·¸ í•¨ìˆ˜ì˜ ë™ì‘ì„ í™•ì¥, ë³€í˜•, ë˜ëŠ” ë‹¨ìˆœí™”í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. \n",
    "ë˜í¼ í•¨ìˆ˜ëŠ” ê¸°ì¡´ì˜ í•¨ìˆ˜ í˜¸ì¶œ ì „ì— ì¶”ê°€ì ì¸ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê±°ë‚˜, í•¨ìˆ˜ì˜ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‘ì—…ì„ ì¶”ê°€í•˜ê³ ì í•  ë•Œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter \n",
    "# operator.itemgetterëŠ” ì£¼ë¡œ sortedì™€ ê°™ì€ í•¨ìˆ˜ì˜ key ë§¤ê°œë³€ìˆ˜ì— ì ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ëª¨ë“ˆ\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def length_function(text):  # í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):  # ë‘ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ê³±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"ì´ë¦„ ì•ì— _ê°€ ë¶™ì–´ ìˆìœ¼ë¯€ë¡œ ë‚´ë¶€ìš©(private) í•¨ìˆ˜ë¡œ ì„¤ê³„ëœ ê²ƒ\"\"\"\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(  # 2ê°œì˜ ë”•ì…”ë„ˆë¦¬ ì¸ìë¥¼ ë°›ëŠ” í•¨ìˆ˜ë¡œ ì—°ê²°í•˜ëŠ” wrapper í•¨ìˆ˜\n",
    "    _dict,\n",
    "):  # ë”•ì…”ë„ˆë¦¬ì—ì„œ \"text1\"ê³¼ \"text2\"ì˜ ê¸¸ì´ë¥¼ ê³±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\" \n",
    "    í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ _dictë¥¼ ì¸ìë¡œ ë°›ê³ , \n",
    "    ê·¸ ë”•ì…”ë„ˆë¦¬ ë‚´ì—ì„œ text1ê³¼ text2ë¼ëŠ” ë‘ ê°œì˜ í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°’ë“¤ì„ ì¶”ì¶œí•´\n",
    "    _multiple_length_function í•¨ìˆ˜ì— ì „ë‹¬\n",
    "    \"\"\"\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}?\")\n",
    "# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ì²´ì¸ ìƒì„±\n",
    "chain1 = prompt | model\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"input_1\") | RunnableLambda(length_function), # 3\n",
    "        \"b\": {\"text1\": itemgetter(\"input_1\"), \"text2\": itemgetter(\"input_2\")} | RunnableLambda(multiple_length_function), # 9\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 + 9 = 12'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì£¼ì–´ì§„ ì¸ìë“¤ë¡œ ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "chain.invoke({\"input_1\": \"bar\", \"input_2\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @chain ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ Runnable êµ¬ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@chain ë°ì½”ë ˆì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ì„ì˜ì˜ í•¨ìˆ˜ë¥¼ ì²´ì¸ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŠ” í•¨ìˆ˜ë¥¼ RunnableLambdaë¡œ ë˜í•‘í•˜ëŠ” ê²ƒê³¼ ê¸°ëŠ¥ì ìœ¼ë¡œ ë™ì¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{topic} ì— ëŒ€í•´ ì§§ê²Œ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{sentence} ë¥¼ emojië¥¼ í™œìš©í•œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œê¸€ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom_chain í•¨ìˆ˜ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì •ì˜ ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@chain ë°ì½”ë ˆì´í„°ë¡œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ë°ì½”ë ˆì´íŒ… í•˜ë©°, ë°ì½”ë ˆì´íŒ…ì„ í†µí•´ í•¨ìˆ˜ë¥¼ Runnable í•œ ê°ì²´ë¡œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def custom_chain(text):\n",
    "    # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸, ChatOpenAI, ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    chain1 = prompt1 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    output1 = chain1.invoke({\"topic\": text})\n",
    "\n",
    "    # ë‘ ë²ˆì§¸ í”„ë¡¬í”„íŠ¸, ChatOpenAI, ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    chain2 = prompt2 | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "    # ë‘ ë²ˆì§¸ ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ íŒŒì‹±ëœ ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ì „ë‹¬í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return chain2.invoke({\"sentence\": output1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom_chainì€ ì´ì œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´(runnable)ì´ë¯€ë¡œ, invoke() ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "LangSmith ì¶”ì  ê¸°ë¡ì„ í™•ì¸í•´ ë³´ë©´, custom_chain ì¶”ì  ì •ë³´ê°€ ìˆì„ ê²ƒì´ë©°, ê·¸ ì•„ë˜ì—ëŠ” OpenAI í˜¸ì¶œì´ ì¤‘ì²©ë˜ì–´ ìˆì„ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŒâœ¨ ì–‘ìì—­í•™ì˜ ë§¤ë ¥ì— ë¹ ì ¸ë³´ì„¸ìš”! ğŸ”ğŸ’«\n",
      "\n",
      "ì–‘ìì—­í•™ì€ ë¬¼ë¦¬í•™ì˜ í•œ ë¶„ì•¼ë¡œ, ì›ìì™€ ì•„ì›ì ì…ìì˜ ì‹ ë¹„í•œ í–‰ë™ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ğŸ§¬ğŸ’¥ ê³ ì „ ë¬¼ë¦¬í•™ê³¼ëŠ” ë‹¬ë¦¬, ì–‘ìì—­í•™ì€ ì…ìê°€ íŠ¹ì •í•œ ìœ„ì¹˜ì— ë™ì‹œì— ì¡´ì¬í•  ìˆ˜ ì—†ê³ , í™•ë¥ ì ì¸ ì„±ì§ˆì„ ê°€ì§„ë‹¤ê³  í•´ìš”! ğŸ²ğŸ”®\n",
      "\n",
      "ì…ìì˜ ìƒíƒœëŠ” ê´€ì¸¡í•˜ê¸° ì „ê¹Œì§€ í™•ì •ë˜ì§€ ì•Šìœ¼ë©°, ì´ëŠ” 'íŒŒë™-ì…ì ì´ì¤‘ì„±'ê³¼ 'ë¶ˆí™•ì •ì„± ì›ë¦¬'ë¡œ ìœ ëª…í•˜ë‹µë‹ˆë‹¤. ğŸŒŠâ¡ï¸âš›ï¸âœ¨ í˜„ëŒ€ ë¬¼ë¦¬í•™ì˜ ê¸°ì´ˆë¥¼ ì´ë£¨ë©°, ë°˜ë„ì²´, ë ˆì´ì €, ì–‘ì ì»´í“¨íŒ… ë“± ë‹¤ì–‘í•œ ê¸°ìˆ ì˜ ë°œì „ì— ê¸°ì—¬í•˜ê³  ìˆì–´ìš”! ğŸ’»âš¡ï¸\n",
      "\n",
      "ì–‘ìì„¸ê³„ì˜ ì‹ ë¹„ë¡œì›€ì„ í•¨ê»˜ íƒí—˜í•´ë´ìš”! ğŸŒŸğŸ”­ #ì–‘ìì—­í•™ #ë¬¼ë¦¬í•™ #ê³¼í•™ì˜ì„¸ê³„ #í˜ì‹  #ê¸°ìˆ ë°œì „ #íŒŒë™ì…ìì´ì¤‘ì„± #ë¶ˆí™•ì •ì„±ì›ë¦¬\n"
     ]
    }
   ],
   "source": [
    "# custom_chainì„ í˜¸ì¶œ\n",
    "print(custom_chain.invoke(\"ì–‘ìì—­í•™\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
