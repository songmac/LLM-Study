{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith API Key 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langsmith로 프로젝트 추적 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "LLM\n"
     ]
    }
   ],
   "source": [
    "from utils import logging\n",
    "\n",
    "logging.langsmith(\"LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 단계는 사용자의 질문에 대한 답변의 질과 자연스러움을 결정짓는 핵심 요소입니다. \n",
    "\n",
    "LLM의 성능은 RAG 시스템의 전체적인 성능과 사용자 만족도에 직접적으로 영향울 끼치며, RAG 시스템을 사용하는 많은 응용 분야에서 매우 중요한 역할을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI GPT-4o 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI 의 GPT-4o 모델 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(참고) 아래는 객체 내부의 속성들을 출력할 수 있는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': None, 'cache': None, 'verbose': False, 'callbacks': None, 'tags': None, 'metadata': None, 'custom_get_token_ids': None, 'callback_manager': None, 'rate_limiter': None, 'disable_streaming': False, 'client': <openai.resources.chat.completions.Completions object at 0x000002CA48241A10>, 'async_client': <openai.resources.chat.completions.AsyncCompletions object at 0x000002CA48066210>, 'root_client': <openai.OpenAI object at 0x000002CA47CEADD0>, 'root_async_client': <openai.AsyncOpenAI object at 0x000002CA4824E810>, 'model_name': 'gpt-3.5-turbo', 'temperature': 0.7, 'model_kwargs': {}, 'openai_api_key': SecretStr('**********'), 'openai_api_base': None, 'openai_organization': None, 'openai_proxy': None, 'request_timeout': None, 'max_retries': 2, 'presence_penalty': None, 'frequency_penalty': None, 'seed': None, 'logprobs': None, 'top_logprobs': None, 'logit_bias': None, 'streaming': False, 'n': 1, 'top_p': None, 'max_tokens': None, 'tiktoken_model_name': None, 'default_headers': None, 'default_query': None, 'http_client': None, 'http_async_client': None, 'stop': None, 'extra_body': None, 'include_response_headers': False, 'disabled_params': None, 'stream_usage': False}\n"
     ]
    }
   ],
   "source": [
    "# print(llm.__dict__) # __dict__ 메서드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-3.5-turbo\n",
      "Temperature: 0.7\n",
      "Top-p: None\n",
      "Frequency Penalty: None\n",
      "Presence Penalty: None\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Model: {llm.model_name}\")\n",
    "# print(f\"Temperature: {llm.temperature}\")\n",
    "# print(f\"Top-p: {llm.top_p}\")\n",
    "# print(f\"Frequency Penalty: {llm.frequency_penalty}\")\n",
    "# print(f\"Presence Penalty: {llm.presence_penalty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic Claude3 Sonnet 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic은 인공지능(AI) 안전성과 연구에 중점을 둔 미국의 스타트업 기업입니다. \n",
    "\n",
    "다음은 Anthropic에 대한 주요 정보입니다.\n",
    "\n",
    "- 2021년에 설립된 AI 기업 \n",
    "\n",
    "- 본사는 미국 샌프란시스코에 위치 \n",
    "\n",
    "- OpenAI 출신 직원들이 설립 (Daniela Amodei와 Dario Amodei 등) \n",
    "\n",
    "- 공익기업(Public Benefit Corporation)으로 등록\n",
    "\n",
    "그 중 Claude는 Anthropic의 대표적인 대규모 언어 모델(LLM) 제품군 입니다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "pip install langchain-anthropic\n",
    "export ANTHROPIC_API_KEY=\"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Anthropic 의 Claude 모델 을 생성합니다.\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='claude-3-sonnet-20240229' anthropic_api_url='https://api.anthropic.com' anthropic_api_key=SecretStr('') model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬 모델(llama3-8b) 활용"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ollama pull mistral:v0.3\n",
    "pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# LangChain이 지원하는 Ollama(로컬) 모델을 사용합니다.\n",
    "llm = ChatOllama(model=\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3:8b'\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
